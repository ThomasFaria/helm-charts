nameOverride: ""
fullnameOverride: "llm-serving"
podAnnotations: {}

deployment:
  image:
    repository: vllm/vllm-openai
    pullPolicy: IfNotPresent
    tag: "v0.6.4"
  hftoken: your_token
  args: 
    model: meta-llama/Llama-3.2-3B-Instruct
    memoryutilization: 0.8
    dtype: half
    maxModelLen: 8208

resources:
  gpu:
    number: 1

service:
  port:
    number: 8000


ingress:
    enabled: false
    className: ""
    annotations: 
      nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
    hostname: "charts.example"

s3:
  enabled: true # Set to true to use S3
  modelHfBucket: ""
  accessKeyId: ""
  endpoint: ""
  defaultRegion: ""
  secretAccessKey: ""
  sessionToken: ""